# get machine IP address
ip addr show eth0 | grep inet | awk '{ print $2; }' | sed 's/\/.*$//'
rservices command

curl 'stage-rservices.int.atti.com/find_listings?q=coffee+and+tea&g=pasadena+ca' -- **/Distance
Search for listings: http://stage-rservices.int.atti.com/v1/find_listings?q=pizza&g=glendale+ca
You can also add the sponsored_results=0 parameter if you don't want advertisers to show up in the search results.
Lookup a listing by ypid: http://stage-rservices.int.atti.com/v1/getListingsByYpid?q=1234
Lookup a listing by listing id: http://stage-rservices.int.atti.com/v1/getListingsById?q=1234
Jacob's tool for JSON response: https://github.com/jdunphy/print_json_response




============ address override commands =============
check  perl code address_canon.pl:
printf '{"city":"glendale","zip5":"91203","street_address":"PO BOX 1234"}\n’      |         /home/t/geo/usps-std/current/dist/Production/bin/address_canon.pl 
Verify standardizer without geolocation
cd /home/t/geo/usps-std/current/dist/Production/bin
./fflcanon.pl
Verify standardizer and geocoder
printf '%%msg)flds)zip5|street_address\n35901|230 4TH ST W\n' | socat - TCP:np227.wc1:3108
http://ast.atti.com/standardizer/index?lookup[street_address]=1434+S+Powerline+Rd&lookup[city]=Pompano+Beach+&lookup[state]=FL&lookup[zip5]=33069&llokup[submit]
also check geosvc
http://prod177.wc1.yellowpages.com:7770/standardize/address?street_address=611+N+Brand+Blvd+Suite+550&zip5=91203
Create override
cd /home/t/geo/geocoder/shared/data/addr_override/cur/2010/06
sudo -u t cp DT-5151.json AS-809.json
sudo -u t vi DT-5049.json
Rertart service ( repackage files)
tpkg --qa
tpkg --restart geosvc-core
Test the code
printf '%%msg)flds)zip5|street_address\n35901|230 4TH ST W\n' | socat - TCP:np227.wc1:3108
Submit
**bad setenv SVN_EDITOR vi
**bad sudo setenv SVN_EDITOR vi
export SVN_EDITOR="/usr/bin/vim"
echo $SNV_EDITOR
vim .bash_profile
source .bash_profile
echo $SVN_EDITOR
cd /Users/fhamedy/geo/addr_override/trunk/cur/2010/06
svn add AS-809.json
svn commit AS-809.json


============------ Other commands ------==============================================

------ openjump ------
cd /Users/fhamedy/Downloads/openjump-1.3.1/bin
./openjump.sh
if any error occurs you may need to set some paths :
cd Downloads/openjump-1.3.1/bin/
JUMP_HOME="/Users/fhamedy/Downloads/openjump-1.3.1"
JUMP_LIB="/Users/fhamedy/Downloads/openjump-1.3.1/lib/"
CLASSPATH=/Users/fhamedy/Downloads/openjump-1.3.1/lib/jts-1.10.jar:$CLASSPATH
./openjump.sh------ Dbf file ------
cd /Users/fhamedy/Downloads/javadbf-0.4.0/
java -cp ./bin:./ JavaDBFReaderTest /Users/fhamedy...
java -cp /Users/fhamedy/Downloads/javadbf-0.4.0/bin:./ JavaDBFReaderTest StIntersections.dbf 1
------ unicorn ------
ps -fade |grep unicorn
------ MOGWAI ------
MOG_DOMAIN=geo mogwai get navteq-dvd_2010q1/D3AM10100ND3000ADX00.tar
MOG_DOMAIN=geo mogwai ls nav
MOG_DOMAIN=geo mogwai ls usps
------ ETL ------
printf '%%msg)flds)zip5|street_address\n35901|230 4TH ST W\n' | socat - TCP:np227.wc1:3108
printf '%%msg)flds)zip5|street_address\n35901|230 S 4th street w\n' | socat - TCP:geosvc.wc1:3108
printf '%%msg)flds)zip5|street_address\n35901|230 S 4TH ST\n' | socat - TCP:geosvc.wc1:3108
printf '%%msg)flds)zip5|street_address\n75039|2820 N Oconner rd\n' | socat - TCP:np227.wc1:3108
printf '%%msg)flds)zip5|street_address\n75039|2820 Riverside Dr Irving TX\n' | socat - TCP:np227.wc1:3108
printf '%%msg)flds)zip5|street_address\n75062|2820 n o connor rd Irving tx\n' | socat - TCP:prod35.wc1:3108=========== tpkg ==================================================
Starting and stopping tpkg service on remote machines
tpkg --start geosvc-core -s np228.wc1.yellowpages.com
=========== Linux ==================================================

to combine all csv files in a folder into one single file:
cat *.csv > codepoint.csv

find . ! -readable -prune
to avoid "Permission denied"
AND do NOT suppress (other) error messages
AND get exit status 0 ("all files are processed successfully")
diff -EBbiwry -W 400 --suppress-common-lines folder1 folder2

To add line breaks to a file (like usps data files )  use this command 
   fold -w 130 -s  ctystate.txt 
   w is the number of characters -s means don’t split words.


- Find and copy to another folder
sudo find /home/t/geo/ -size -2048k -exec cp -r {} ./ \;

Executing a command on each folder by cd to the folder (folder  names being lab01,lab02,lab03) and command being scons


find "$HOME"/gpgn302/lab* -type d -exec bash -c 'cd "$1"; scons -c' -- {} \;
Adding text to file 
sed can add text to the begining of a file:
$ sed -i '1s/^/<added text> /' file



On mac in regex \t does not work instead use the keys to insert a tab :
 Ctrl+V and then press Tab
Linux search command for flat file content
find / -type f -exec grep mysearchpattern {} /dev/null \;
find . -name "*.c" -print | xargs grep "main("
linux finding file by modification between 30 day ago and 60 days ago
find / -ctime +30 -ctime -60
to find and search for 'string' in files *.xml : grep -iR 'pigz' *.xmlto find and search for files named 'string*pattern' in root dir :find / -name 'navteq_dvd*'
to find and search for files in home dir where ---------170***********150----------------------now---
---"modified older than 150 days ago"
---but
---"modified less than 170 days ago" :find ~ -mtime +150 -mtime -170 -daystart
combining these:
find ~ -mtime +150 -mtime -170 -daystart -iname "*.xml”
kill all proceesses by a user
kill `ps -fu $User | awk 'NR != 1 {print $2}'`

sort testRestults150k3.txt | cut -f5,9 -d"|"

grep -iR "zzz" existingfile.txt > newfilezzz.txt

sed --in-place '/zzz/d' testRestults150k3.txt 

Grep -c “zzz” filename   count occurrences of text in file
-------------------
the most common way of finding “10 most common words” in a file, 
tr -c '[:alnum:]' '[\n*]' < test.txt | sort | uniq -c | sort -nr | head  -10
- - - -  fixed width text files  - - - - - - - - - - - - - - - 
--- Using awk to read fields and print them out
awk -v FIELDWIDTHS='1 10 4 2 2' -v OFS=',' '
    { $1=$1 ""; print }
' data.txt
--- OR     this code snippet  ---------
awk '{sub(".","&,");sub(".{12}","&,");sub(".{17}","&,");sub(".{20}","&,")} 1' data.txt                                         
--- using awk to print column 1 if column 5 is not equal to xxx
awk 'BEGIN{FS=OFS=" "}{if($5 != 25) print $1}' bla.txt    

- - - -      - - - - - - - - - - - - - - - - - - - - - - - - -  - -


=============================================================

/home/t/geo/usps-std/current/dist/Production/bin/fflcanon.pl
---
cd /home/t/geo/geocoder/shared/data/addr_override/cur/2010/10
cd /home/t/geo/geosvc/current/lib/
cd /home/t/var/tpkg/tmp/geosvc-core-1.1-29.10888.0/tpkg/root/home/t/geo
---
sudo -u t cp AS-1304.json AS-1152.json
sudo -u t vi AS-1152.json
sudo -u t svn update
---
sudo -u t vi addr_override.rb
sudo -u t vi titleize_address.rb
sudo find . -name address_override.rb
---
svn co https://subversion.flight.yellowpages.com/data/geo
---
tpkg --make geocore/tpkg
tpkg --qa
tpkg --restart geosvc-core
===============================================================
Splitting a large file into smaller files.
Syntax
split [-linecount | -l linecount ] [ -a suffixlength ] [file [name] ]
split -b n [k | m] [ -a suffixlength ] [ file [name]]
Examples
split -b 22 newfile.txt new
Split the file "newfile.txt" into three separate files called newaa, newab and newac each file the size of 22.
split -l 300 file.txt new
Split the file "newfile.txt" into files beginning with the name "new" each containing 300 lines of text each

===============================================================

 SED command in Mac ox is sooooooo crappy it requires a newline to terminate the sub-commands ( not \n ) a real line return in the middle of the sed command:

Strictly speaking, the POSIX specification for sed requires a newline after a\:
[1addr]a\
text


Write text to standard output as described previously.
This makes writing one-liners a bit of a pain, which is probably the reason for the following GNU extension to the a, i, and c commands:
As a GNU extension, if between the a and the newline there is other than a whitespace-\sequence, then the text of this line, starting at the first non-whitespace character after the a, is taken as the first line of the text block. (This enables a simplification in scripting a one-line add.) This extension also works with the i and c commands.
Thus, to be portable with your sed syntax, you will need to include a newline after the a\somehow. The easiest way is to just insert a quoted newline:
$ sed -e 'a\
> text'


(where $ and > are shell prompts). If you find that a pain, bash has the $' ' quoting syntax for inserting C-style escapes, so just use
sed -e 'a\'$'\n''text'
My example was  sed -i '' '1i\'$'\n var hoooohoooo [ ' SD.json Notice the  blank single  ''  quotes after "sed -i”  they are necessary when using -i (to mean a fictional output file since this is in-place edit )
find . -type f -name '*.json' -exec sed -i '' '1i\'$'\n var nei = [ '  {} \;

===============================================================

ON MAC OS X
in regex \t does not work instead use the
Keyboard keys to insert a tab :
Ctrl+V and then press Tab

ON MAC OS X
if setenv returns "Command not found" then the launchctrl command can be used:
launchctl setenv GOROOT /usr/local/go
launchctl setenv GOPATH /Users/yourname/go
In this example , deleting tabs
sed -i -e '/ $/d' lon_lat_ypid_nei_TAB.csv

======================================================
When Creating geosvc TPKG:
just the files from the
~/geocore/tpkg/root/home/t/…
folder should be included into the tpkg
NOT
NOT THE FILES FROM
~/geocore/root/home/t/...

========================================================================================


$ tpkg -x .
tpkg --make geocore/tpkg
tpkg -i geosvc-core-1.1-38.tpkg
========================================================================================
to find without permission errors

find / ! -readable -prune -name *jdbc*.jar


to find and search for 'string' in files *.xml : grep -iR 'pigz' *.xml
to find and search for files named 'string*pattern' in root dir : find / -name 'navteq_dvd*'
to find and search for files in home dir where ---------170***********150----------------------now---
---"modified older than 150 days ago"
---but
---"modified less than 170 days ago" :
find ~ -mtime +150 -mtime -170 -daystart
combining these:
find ~ -mtime +150 -mtime -170 -daystart -iname "*.xml"

====================================

Remove lines with a keyword from many files
sed -i -e '/\| \[\]/d' ypidlatlonnei_small.txt
-------------------------
Remove first 2 characters of each line
sed 's/^..//' file1.txt > file2.txt
-------------------------
Length of the longest line
awk '{ if (length($0) > max) max = length($0) } END { print max }’ ypidlatlonnei_small.txt
-------------------------
# delete duplicate, consecutive lines from a file (emulates "uniq").
# First line in a set of duplicate lines is kept, rest are deleted.
sed '$!N; /^\(.*\)\n\1$/!P; D'
-------------------------
# delete duplicate, nonconsecutive lines from a file. Beware not to
# overflow the buffer size of the hold space, or else use GNU sed.
sed -n 'G; s/\n/&&/; /^\([ -~]*\n\).*\n\1/d; s/\n//; h; P'
-------------------------
Here's one way to do it with sort
cat file.txt | sort -u > filename.txt


===== merge by content ===============================
Another use case is when you need to merge only specific parts of certain files depending on some condition. This is especially useful for me when I have to analyze several large log files, but am only interested in certain messages or lines. So, I will need to extract the important log messages based on some criteria from several log files and save them in a different file while also maintaining or preserving the order of the messages.
Though you can do this using cat and grep commands, you can do it with just the grep command as well.
bash$ grep -h "[Error]" logfile*.log > onlyerrors.log
The above will extract all the lines that match the pattern [Error] and save it to another file. You will have to make sure that the log files are in order when using the regular expression to match them, as mentioned earlier in the post.

====================================







